# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19qtikVkSmt1SFaB3frgKwCWsCHfrlScW
"""

#%%
import numpy as np
path='E:\\pyramidal\\jingCode\\cnnData\\'

from data_generator import DataGenerator

from keras.optimizers import Adam
from keras.layers.core import Dropout
from keras.layers import Conv2D, MaxPooling2D, Input, UpSampling2D,concatenate, BatchNormalization
from keras.callbacks import ModelCheckpoint
from keras.models import Model

import keras
from keras import backend as K
import tensorflow as tf

inputs = Input(shape=(256,256,1))
conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)
conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)
#bn1= BatchNormalization()(conv1)
pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)
conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)
conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)
#bn2= BatchNormalization()(conv2)
pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)
conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)
conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)
#bn3= BatchNormalization()(conv3)
pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)
conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)
conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)
#bn4= BatchNormalization()(conv4)
drop4 = Dropout(0.5)(conv4)
pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)

conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)
conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)
#bn5= BatchNormalization()(conv5)
drop5 = Dropout(0.5)(conv5)

up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))
merge6 = concatenate([drop4,up6],axis = 3)
conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)
conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)
#bn6= BatchNormalization()(conv6)

up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))
merge7 = concatenate([conv3,up7],axis = 3)
conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)
conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)
#bn7= BatchNormalization()(conv7)

up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))
merge8 = concatenate([conv2,up8],axis = 3)
conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)
conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)
#bn8= BatchNormalization()(conv8)

up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))
merge9 = concatenate([conv1,up9], axis = 3)
conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)
conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)
conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)
model=Model(inputs,outputs=conv10)
print(model.summary())
print('training model ...')
filepath="{epoch:02d}-{val_loss:.4f}.hdf5"
modelcheckpoint=ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto',period=5)
model.compile(optimizer = Adam(lr = 1e-5),loss='binary_crossentropy')
myGenerator = DataGenerator(20,dict(rotation_range=20,
                    shear_range=0.15,
                    zoom_range=0.15,
                    vertical_flip=True,
                    horizontal_flip=True,
                    fill_mode='nearest',
                    brightness_range=[0.5, 1.5]),200)

myGenerator.random_slice_data()
train_generator=myGenerator.trainGenerator()
val_data=myGenerator.valData()
model_info=model.fit_generator(
    train_generator,
    steps_per_epoch=200,
    epochs=20,verbose=1,callbacks=[modelcheckpoint],validation_data=val_data)

import matplotlib.pyplot as plt

def plot_model_history(model_history):
    # summarize history for loss
    plt.plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])
    plt.plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['train', 'val'], loc='best')
    plt.savefig('LOSS.png')
    plt.show()
plot_model_history(model_info)
#%%
#from keras.models import load_model
#model=load_model(path+'10-0.9726.hdf5')
#
#
#result=model.predict(image[:10,:,:,:], batch_size=5, verbose=1)
#result[result>=0.5]=1
#result[result<0.5]=0
##%%
#import matplotlib.pyplot as plt
#
#for i in range(10):
#    plt.imshow(result[i,:,:,0], cmap='gray') 
#    plt.show()



#from sklearn.model_selection import KFold
#
#model.compile(optimizer = Adam(lr = 1e-4),loss='binary_crossentropy',metrics=['accuracy'])
#cv=KFold(n_splits=5)
#cv_score=[]
#for train, test in cv.split(image):
#    x_train_cv,x_test_cv=image[train],image[test]
#    y_train_cv, y_test_cv=mask[train],mask[test]
#    model_info=model.fit(x_train_cv,
#                     y_train_cv,
#                     batch_size=5,
#                     epochs=5,
#                     verbose=1)
#    scores = model.evaluate(x_test_cv, y_test_cv, verbose=1)
#    print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
#    cv_score.append(scores[1])
#print("%.4f%% (+/- %.4f%%)" % (np.mean(cv_score), np.std(cv_score)))

#def focal_loss(y_true, y_pred):
#    gamma = 2.0, alpha = 0.25
#    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
#    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
#    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))
#
## Compile our model
#
#model.compile(loss=[focal_loss], optimizer=Adam(lr = 1e-5)) 
#model_info=model.fit(image,
#                      mask,
#                      batch_size=5,
#                      nb_epoch=12,
#                      verbose=1,
#                      validation_split=0.2,
#                     callbacks=[modelcheckpoint])